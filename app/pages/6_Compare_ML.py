# ğŸ“„ pages/7_Model_Comparison.py
import streamlit as st
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import xgboost as xgb
import catboost as cb
import plotly.graph_objects as go

st.set_page_config(page_title="ğŸ¤– ×”×©×•×•××ª ××•×“×œ×™×", layout="wide")
st.title("ğŸ¤– ×”×©×•×•××ª ××•×“×œ×™×: LightGBM, XGBoost, CatBoost")

FILE_NAME = os.path.join("..", "logs", "signal_backtest_results_for_AI.xlsx")
if not os.path.exists(FILE_NAME):
    st.warning(f"×”×§×•×‘×¥ {FILE_NAME} ×œ× × ××¦×")
    st.stop()

try:
    df = pd.read_excel(FILE_NAME)
except Exception as e:
    st.error(f"×©×’×™××” ×‘×˜×¢×™× ×ª ×”×§×•×‘×¥: {e}")
    st.stop()

# ×¢×™×‘×•×“ ×–××Ÿ
if 'signal_time' in df.columns:
    df['signal_time'] = pd.to_datetime(df['signal_time'])
    df['hour'] = df['signal_time'].dt.hour
    df['day_of_week'] = df['signal_time'].dt.dayofweek

# ×”×¦×œ×—×”: âœ… TP Hit ×‘×œ×‘×“
valid_results = ['âœ… TP Hit', 'âŒ SL Hit']
df = df[df['×ª×•×¦××”'].isin(valid_results)]
df['success'] = df['×ª×•×¦××”'] == 'âœ… TP Hit'

# Label Encoding ×œ×§×˜×’×•×¨×™×•×ª
encoders = {}
for col in ['symbol', 'strategy', 'Symbol_category']:
    if col in df.columns:
        enc = LabelEncoder()
        df[col] = enc.fit_transform(df[col].astype(str))
        encoders[col] = enc

# ×ª× ××™ ××’××” EMA
if 'ema_short' in df.columns and 'ema_long' in df.columns:
    df['trend_ema'] = (df['ema_short'] > df['ema_long']).astype(int)

# ×ª×›×•× ×•×ª
feature_cols = [col for col in [
    'RSI', 'MACD', 'ADX', 'hour', 'day_of_week',
    'symbol', 'strategy', 'trend_ema',
    'Volume_avg', 'Volatility', 'Trend_strength', 'BB_width', 'Symbol_category']
    if col in df.columns]

if not feature_cols:
    st.error("×œ× × ××¦××• ××¡×¤×™×§ ×¢××•×“×•×ª ×—×™×–×•×™")
    st.stop()

# ×”×›× ×ª ×“××˜×”
model_df = df.dropna(subset=feature_cols + ['success'])
X = model_df[feature_cols]
y = model_df['success']

# ×—×œ×•×§×” ×œ×¡×˜×™×
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ×¤×•× ×§×¦×™×” ×œ××™××•×Ÿ ×•×”×¢×¨×›×”
results = {}

def evaluate_model(name, model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    report = classification_report(y_test, y_pred, output_dict=True)
    auc = roc_auc_score(y_test, y_proba)
    results[name] = {
        'accuracy': report['accuracy'],
        'auc': auc,
        'precision': report['True']['precision'],
        'recall': report['True']['recall'],
        'f1': report['True']['f1-score']
    }

# ×”×©×•×•××ª ××•×“×œ×™×
with st.spinner("××××Ÿ ××•×“×œ×™×..."):
    evaluate_model("LightGBM", lgb.LGBMClassifier())
    evaluate_model("XGBoost", xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'))
    evaluate_model("CatBoost", cb.CatBoostClassifier(verbose=0))

# ×ª×¦×•×’×” ×’×¨×¤×™×ª
st.subheader("ğŸ“Š ×”×©×•×•××ª ×‘×™×¦×•×¢×™×")
metrics = ['accuracy', 'auc', 'precision', 'recall', 'f1']
fig = go.Figure()

for metric in metrics:
    fig.add_trace(go.Bar(
        x=list(results.keys()),
        y=[results[model][metric] for model in results],
        name=metric
    ))

fig.update_layout(barmode='group', title="×”×©×•×•××ª ××•×“×œ×™× ×œ×¤×™ ××“×“×™×", xaxis_title="××•×“×œ", yaxis_title="×¢×¨×š")
st.plotly_chart(fig, use_container_width=True)

# ×˜×‘×œ×ª ×ª×•×¦××•×ª
st.subheader("ğŸ“‹ ×˜×‘×œ×” ××¡×›××ª")
st.dataframe(pd.DataFrame(results).T.round(3))
